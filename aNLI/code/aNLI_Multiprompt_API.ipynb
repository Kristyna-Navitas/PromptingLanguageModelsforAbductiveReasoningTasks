{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81e6ccab",
   "metadata": {},
   "source": [
    "# Hugging Face API inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d59513",
   "metadata": {},
   "source": [
    "### Setting up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54b71569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "#import re\n",
    "\n",
    "from getpass import getpass\n",
    "\n",
    "from langchain import HuggingFaceHub\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.chains import SequentialChain\n",
    "\n",
    "#from Levenshtein import distance \n",
    "\n",
    "#from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6ddbf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl(path):\n",
    "    lines = []\n",
    "    with open(path) as file:\n",
    "        lines = file.read().splitlines()\n",
    "\n",
    "    return pd.DataFrame([json.loads(line) for line in lines])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aca1656c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data length: 3059\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>story_id</th>\n",
       "      <th>obs1</th>\n",
       "      <th>obs2</th>\n",
       "      <th>hyp1</th>\n",
       "      <th>hyp2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>87aa0983-9b84-48b1-86ff-160b1567487c-1</td>\n",
       "      <td>Jane was a professor teaching piano to students.</td>\n",
       "      <td>Jane spent the morning sipping coffee and reading a book.</td>\n",
       "      <td>Two of Jane's students were early for their lessons.</td>\n",
       "      <td>None of Jane's students had a lesson that day.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dfc8584e-13fe-4e26-bdf6-2485e90ef29d-1</td>\n",
       "      <td>Nate had the summer off before college.</td>\n",
       "      <td>Nate's last summer before college was a total blast!</td>\n",
       "      <td>Nate spent the summer traveling and partying.</td>\n",
       "      <td>Nate decided to spend the entire summer working in the Mines.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 story_id  \\\n",
       "0  87aa0983-9b84-48b1-86ff-160b1567487c-1   \n",
       "1  dfc8584e-13fe-4e26-bdf6-2485e90ef29d-1   \n",
       "\n",
       "                                               obs1  \\\n",
       "0  Jane was a professor teaching piano to students.   \n",
       "1           Nate had the summer off before college.   \n",
       "\n",
       "                                                        obs2  \\\n",
       "0  Jane spent the morning sipping coffee and reading a book.   \n",
       "1       Nate's last summer before college was a total blast!   \n",
       "\n",
       "                                                   hyp1  \\\n",
       "0  Two of Jane's students were early for their lessons.   \n",
       "1         Nate spent the summer traveling and partying.   \n",
       "\n",
       "                                                            hyp2  label  \n",
       "0                 None of Jane's students had a lesson that day.      2  \n",
       "1  Nate decided to spend the entire summer working in the Mines.      1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aNLIPath = '/home/navitas/Documents/irsko/C3_project_CT5108/data/anli/'\n",
    "aNLI_test = load_jsonl(aNLIPath+'test.jsonl')\n",
    "label_test = pd.read_csv(aNLIPath+'test-labels.lst', header=None, names=['label'])\n",
    "aNLI_test = aNLI_test.join(label_test)\n",
    "print('test data length:', len(aNLI_test))\n",
    "\n",
    "aNLI_train = load_jsonl(aNLIPath+'train.jsonl')\n",
    "label_train = pd.read_csv(aNLIPath+'train-labels.lst', header=None, names=['label'])\n",
    "aNLI_train = aNLI_train.join(label_train)\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "aNLI_test.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d0f86e",
   "metadata": {},
   "source": [
    "### Logging to HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77b82f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "# logging in\n",
    "HUGGINGFACEHUB_API_TOKEN = getpass()\n",
    "\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HUGGINGFACEHUB_API_TOKEN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d2da3c",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee172bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = 0.1\n",
    "length = 2000\n",
    "tokens = 64\n",
    "\n",
    "# FLAN-T5 model - 11.3B parameters; xl - 3B\n",
    "llm_t5 = HuggingFaceHub(repo_id=\"google/flan-t5-xxl\", model_kwargs={\"max_new_tokens\": tokens})#, \"max_length\": length})\n",
    "\n",
    "# Falcon model\n",
    "llm_falcon = HuggingFaceHub(repo_id=\"tiiuae/falcon-7b\", model_kwargs={\"max_new_tokens\": tokens})#, \"max_length\": length})\n",
    "llm_falcon_inst = HuggingFaceHub(repo_id=\"tiiuae/falcon-7b-instruct\", model_kwargs={\"max_new_tokens\": tokens})#, \"max_length\": length})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5a3b20-6ee9-47bd-a895-1c36c63639ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prediction = give_prediction(llm_falcon_inst, overall_chain, aNLI_test, 'falcon_chain2_pokus_likely_3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ee4403-6a8c-4b2b-951f-eb638424258d",
   "metadata": {},
   "source": [
    "## Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "da97804c-69a0-4144-80cc-e8d3c98165e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = llm_falcon_inst # llm_t5 llm_falcon_inst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "adff94de-a295-4f60-ab79-8e8f65737f5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Example of contradiction: Jenny wanted cupcakes. The bakery had no cupcakes left. She bought them all.\\nExample of contradiction: It was a swelteringly hot day. Everyone was given soup. They all loved the cool treat!\\nExample of contradiction: Jake was at the park. Jake saw a tree. He chased after it but never caught it.\\nExample of contradiction: Alice has a ball. Alice kept the ball. Alice does not have the ball.\\nNow decide if there is a logical or commonsense contradiction in the following text and explain it.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# i could add a chain that answers why is something incoherent or less likely...\n",
    "\n",
    "# ending sentence only and then together\n",
    "ending_sentence_few = \"\"\"Instruction: The task is to select one and only one of two provided sentences, nothing elese, \n",
    "to logically and coherently continue the given context. I will give you a few examples.\n",
    "\n",
    "Context is: Alice does not have the ball. Sentence A is: Alice kept the ball. Sentence B: Alice gave the ball to Bill.\n",
    "Response: Alice does not have the ball. That is because Alice gave the ball to Bill. Sentence B explains the context better.\n",
    "Context is: They all loved the cool treat! Sentence A is: Everyone was given ice cream. Sentence B is: Everyone was given soup. \n",
    "Response: They all loved the cool treat! That is because Everyone was given ice cream. Sentence A explains the context better.\n",
    "Context is: She bought them all. Sentence A is: The bakery had no cupcakes left. Sentence B is: The bakery only had three cupcakes left.\n",
    "Response: She bought them all. That is because The bakery only had three cupcakes left. Sentence B explains the context better.\n",
    "\n",
    "Context is: {O2} Sentence A is: {H1} Sentence B is: {H2} \n",
    "Response:\n",
    "\"\"\"\n",
    "ending_sentence_template_few = PromptTemplate(input_variables=[\"O2\", \"H1\", \"H2\"], template=ending_sentence_few)\n",
    "ending_sentence_few = LLMChain(llm=llm, prompt=ending_sentence_template_few, output_key=\"answerEF\")\n",
    "\n",
    "consistency = \"\"\"Instruction: The task is to decide if the sentences in the given text are consistent (answer \"yes\") \n",
    "or if they contain a logical with each other (answer \"no\"). Then provide an explanation.\n",
    "Text: {O1} {answerEF}\n",
    "Response:\n",
    "\"\"\"\n",
    "consistency_template = PromptTemplate(input_variables=[\"O1\", \"answerEF\"], template=consistency)\n",
    "consistency_end_chain = LLMChain(llm=llm, prompt=consistency_template, output_key=\"consistency_O1\")\n",
    "\n",
    "\n",
    "###############\n",
    "\n",
    "\n",
    "# out: Example of contradiction: I was walking to the mall. On the way, I realized I forgot my phone. I almost cracked my phone.\n",
    "consistencyBoth = \"\"\"Instruction: There are two independent texts, B and A. The task is to decide which one of the texts \n",
    "is not logically consistent and contains a contradiction. Consider only the information in the text. \n",
    "Now decide which of those two texts contains contradictions.\n",
    "Text A: {O1} {H1} {O2} Text B: {O1} {H2} {O2} \n",
    "Response:\n",
    "\"\"\"\n",
    "consistency_templateBoth = PromptTemplate(input_variables=[\"O1\", \"O2\", \"H1\", \"H2\"], template=consistencyBoth)\n",
    "consistency_both = LLMChain(llm=llm, prompt=consistency_templateBoth, output_key=\"consistency_both\")\n",
    "\n",
    "\n",
    "\n",
    "###############\n",
    "\n",
    "inconsistency = \"\"\"Instruction: The task is to explain the inconsistency and logical fallacy between the sentences of given text.\n",
    "Answer with phrase \"It is because ___ \", do not use the words \"inconsistency\" and \"logical fallacy\" in the explanation.\n",
    "Text: {O1} {H1} {O2}\n",
    "Response:\n",
    "\"\"\"\n",
    "inconsistency_template = PromptTemplate(input_variables=[\"O1\",\"H1\",\"O2\"], template=inconsistency)\n",
    "inconsistency_chain = LLMChain(llm=llm, prompt=inconsistency_template, output_key=\"inconsistency\")\n",
    "\n",
    "\n",
    "correctness = \"\"\"Instruction: The task is to decide if given explanation is coherent and correct.\n",
    "Explanation: {inconsistency}\n",
    "Response:\n",
    "\"\"\"\n",
    "correctness_template = PromptTemplate(input_variables=[\"inconsistency\"], template=correctness)\n",
    "correctness_chain = LLMChain(llm=llm, prompt=correctness_template, output_key=\"correctness\")\n",
    "\n",
    "\n",
    "likely_expl = \"\"\"Instruction: The task is to decide which one of the explanations is more correct and more likely. \n",
    "Answer with phrase \"Explanation __ is more likely because ___\".\n",
    "The context is: {O1} {O2}\n",
    "Explanation A is: {explanation1}\n",
    "Explanation B is: {explanation2}\n",
    "Response:\n",
    "\"\"\"\n",
    "likely_expl_template = PromptTemplate(input_variables=[\"explanation1\", \"explanation2\", \"O1\", \"O2\"], template=likely_expl)\n",
    "likely_expl_chain = LLMChain(llm=llm, prompt=likely_expl_template, output_key=\"likely_expl\")\n",
    "\n",
    "\n",
    "################\n",
    "# NOT USED\n",
    "\n",
    "likeliness = \"\"\"Instruction: The task is to decide which story (B or A) is more logically coherent and consistent, \n",
    "considering only the information in the story. \n",
    "Story A: {O1} {H1} {O2} Story B: {O1} {H2} {O2}. Which Story (B or A) is more coherent?\"\"\"\n",
    "likeliness_template = PromptTemplate(input_variables=[\"O1\", \"O2\", \"H1\", \"H2\"], template=likeliness)\n",
    "likeliness_chain = LLMChain(llm=llm, prompt=likeliness_template, output_key=\"likeliness\")\n",
    "\n",
    "\n",
    "\n",
    "#  or the people or the things in the text do not match\n",
    "#consistency_one = \"\"\"Instruction: The task is to decide if the sentences in the given text are consistent with each other.\n",
    "#Now decide if there is a logical or commonsense contradiction in the following text and explain it.\n",
    "#Text: {O1} {H1} {O2}\n",
    "#Response:\n",
    "#\"\"\"\n",
    "#consistency_one_template = PromptTemplate(input_variables=[\"O1\", \"O2\", \"H1\"], template=consistency_one)\n",
    "#consistency__one_chain = LLMChain(llm=llm, prompt=consistency_one_template, output_key=\"consistencyOne\")\n",
    "\n",
    "\n",
    "\"\"\"Example of contradiction: Jenny wanted cupcakes. The bakery had no cupcakes left. She bought them all.\n",
    "Example of contradiction: It was a swelteringly hot day. Everyone was given soup. They all loved the cool treat!\n",
    "Example of contradiction: Jake was at the park. Jake saw a tree. He chased after it but never caught it.\n",
    "Example of contradiction: Alice has a ball. Alice kept the ball. Alice does not have the ball.\n",
    "Now decide if there is a logical or commonsense contradiction in the following text and explain it.\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f7c9b6e4-9cc1-4bef-9e8c-73ef81215b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#overall_chain = SequentialChain(chains=[ending_sentence_few, ending_sentence_few2, starting_sentence_few, consistency_chain, same_hyp_chain],\n",
    "#                                input_variables=[\"O1\",\"O2\", \"H1\", \"H2\",\"answerEF\", \"answerSF\", \"answerEF2\" ], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8fef1369-0686-4fc0-9460-843c2b435e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_prediction(model, samples, save):\n",
    "    answers = []\n",
    "    story_id = []\n",
    "    end, consist_end, consist_both, inc1, inc2, cor1, cor2, lik_expl = [],[],[],[],[],[],[],[]\n",
    "    c = 1\n",
    "\n",
    "    for n in samples.index:\n",
    "        obs1 = samples.obs1[n]\n",
    "        obs2 = samples.obs2[n]\n",
    "        hyp1 = samples.hyp1[n]\n",
    "        hyp2 = samples.hyp2[n]\n",
    "\n",
    "        endingF = ending_sentence_few.run({\"O2\":obs2,\"H1\":hyp1,\"H2\":hyp2})\n",
    "        end.append(endingF)\n",
    "        #print(endingF)\n",
    "        CE = consistency_end_chain.run({\"O1\":obs1,\"answerEF\":endingF})\n",
    "        consist_end.append(CE)\n",
    "        #print(C)\n",
    "\n",
    "        \n",
    "        CB = consistency_both.run({\"O1\":obs1, \"O2\":obs2,\"H1\":hyp1,\"H2\":hyp2})\n",
    "        consist_both.append(CB)\n",
    "        #print(CA)\n",
    "\n",
    "        \n",
    "        h1 = inconsistency_chain.run({\"O1\":obs1,\"O2\":obs2, \"H1\":hyp1})\n",
    "        inc1.append(h1)\n",
    "        h2 = inconsistency_chain.run({\"O1\":obs1,\"O2\":obs2, \"H1\":hyp2})\n",
    "        inc2.append(h2)\n",
    "\n",
    "        c1 = correctness_chain.run({\"inconsistency\":h1})\n",
    "        cor1.append(c1)\n",
    "        c2 = correctness_chain.run({\"inconsistency\":h2})\n",
    "        cor2.append(c2)\n",
    "\n",
    "        le = likely_expl_chain.run({\"explanation1\":h1, \"explanation2\":h2, \"O1\":obs1, \"O2\":obs2})\n",
    "        lik_expl.append(le)\n",
    "        \n",
    "        #print(t)\n",
    "        #out = ChainOfThought_chain.run({\"O1\":obs1,\"O2\":obs2,\"H1\":hyp1, \"H2\":hyp2,\"answerA\":consA, \"answerB\":consB, \"likeliness\":lik})\n",
    "        #print('ANSWER', out)\n",
    "        #print('..............')\n",
    "\n",
    "        # with chain i dont know how to get the partial answers to csv\n",
    "        #answers.append(out)\n",
    "        story_id.append(samples.story_id[n])\n",
    "\n",
    "        if c%10 == 0:\n",
    "            pd.DataFrame({'story_id':story_id, \"end\":end, \"end_cons\":consist_end, \"consist\": consist_both, \n",
    "                          \"inconsH1\":inc1, \"inconsH2\":inc2, \"cor1\":cor1, \"cor2\":cor2, \"likEx\":lik_expl}).to_csv(save, index=False)\n",
    "            print(c)\n",
    "        c += 1\n",
    "        \n",
    "\n",
    "    pd.DataFrame({'story_id':story_id, \"end\":end, \"end_cons\":consist_end, \"consist\": consist_both, \n",
    "                \"inconsH1\":inc1, \"inconsH2\":inc2, \"cor1\":cor1, \"cor2\":cor2, \"likEx\":lik_expl}).to_csv(save, index=False)\n",
    "    \n",
    "    print('samples answered: ', len(answers))\n",
    "\n",
    "    return answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802995c6-c069-4f51-81a2-c69722fb4742",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n"
     ]
    }
   ],
   "source": [
    "prediction = give_prediction(llm, aNLI_test, 'falcon_multiprompt_finaly_final.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f52d4a-9b9a-4719-9e82-c17965029d7f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Legacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e487385-17a0-4f59-af77-c99c5fe349ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i could add a chain that answers why is something incoherent or less likely...\n",
    "\n",
    "consistencyA1 = \"\"\"E is: {O1} {H1} C is: {O2} Could E explain C in this context? Answer yes or no and explain why.\"\"\"\n",
    "consistencyA1_template = PromptTemplate(input_variables=[\"O1\", \"O2\", \"H1\"], template=consistencyA1)\n",
    "consistencyA1_chain = LLMChain(llm=llm, prompt=consistencyA1_template, output_key=\"answerA1\")\n",
    "\n",
    "consistencyB1 = \"\"\"The task is to decide if the continuation of the story is coherent with the story and logical. The Story is: {O1} {H1}. Could the Story logically continue with the sentence \"{O2}\"? Answer yes or no and explain why or why not.\"\"\" \n",
    "consistencyB1_template = PromptTemplate(input_variables=[\"O1\",\"O2\", \"H1\"], template=consistencyB1)\n",
    "consistencyB1_chain = LLMChain(llm=llm, prompt=consistencyB1_template, output_key=\"answerB1\")\n",
    "\n",
    "consistencyA2 = \"\"\"E is: {O1} {H2} C is: {O2} Could E explain C in this context? Answer yes or no and explain why.\"\"\"\n",
    "consistencyA2_template = PromptTemplate(input_variables=[\"O1\", \"O2\", \"H2\"], template=consistencyA2)\n",
    "consistencyA2_chain = LLMChain(llm=llm, prompt=consistencyA2_template, output_key=\"answerA2\")\n",
    "\n",
    "consistencyB2 = \"\"\"The task is to decide if the continuation of the story is coherent with the story and logical. The Story is: {O1} {H2}. Could the Story logically continue with the sentence \"{O2}\"? Answer yes or no and explain why or why not.\"\"\" \n",
    "consistencyB2_template = PromptTemplate(input_variables=[\"O1\",\"O2\", \"H2\"], template=consistencyB2)\n",
    "consistencyB2_chain = LLMChain(llm=llm, prompt=consistencyB2_template, output_key=\"answerB2\")\n",
    "\n",
    "\n",
    "likeliness1 = \"\"\" The task is to choose a better and more coherent Explanation (B or A) for the Conclusion.\n",
    "\n",
    "Context: Dan was digging in his yard to put in an extension to his home. Explanation A: Dan hit a pipe. Explanation B: Dan's shovel hit something soft. Conclusion: When he sniffed the air he realized he'd struck a sewage pipe.\n",
    "Answer: Explanation A explains the Conclusion better because the sewage pipe is not soft. Correct explanation is A.\n",
    "\n",
    "Context: Sam got a cold one day. Explanation A: Sam went to the library. Explanation B: He went for a run and his cold got worse. Conclusion: Sam had to take medicine and rest for a week.\n",
    "Answer: The Explanation B explains the Conclusion better because the run worsened the cold so Sam had to rest. Correct explanation is B.\n",
    "\n",
    "Context: Jenny wanted cupcakes. Explanation A: The bakery only had three cupcakes left. Explanation B: The bakery had no cupcakes left. Conclusion: She bought them all.\n",
    "Answer: Explanation A explains the Conclusion better becaues the bakery had cupcakes left so Jenny bought them. Explanation A is better. Correct explanation is A.\n",
    "\n",
    "Context: Jake was at the park. Explanation A: Jake saw a tree. Explanation B: Jake saw a squirrel. Conclusion: He chased after it but never caught it.\n",
    "Answer: Explanation B explains the Conclusion better because when Jake saw a squirrel he chased after it. Correct explanation is B.\n",
    "\n",
    "Context:  When I was younger, I went to summer camp. Explanation A: We ate soup every night. Explanation B: We played cards a lot. Conclusion: My favorite card game was poker.\n",
    "Answer: Explanation B explains the Conclusion better because we played cards alot and my favorite card game was poker are related. Correct explanation is B.\n",
    "\n",
    "Context: It was a swelteringly hot day. Explanation A: Everyone was given ice cream. Explanation B: Everyone was given soup. Conclusion: They all loved the cool treat!\n",
    "Answer: Explanation A explains the Conclusion better because a cool treat could be the ice cream. Correct explanation is A.\n",
    "\n",
    "Context: I was walking to the mall. Explanation A: I tripped on a stick. Explanation B: On the way, I realized I forgot my phone. Conclusion: I almost cracked my phone.\n",
    "Answer: Explanation A explains the Conclusion better because cracking phone better explains that I tripped on a stick. Correct explanation is A.\n",
    "\n",
    "Context: Neil was traveling in Vietnam. Explanation A:Neil decided to teach new things to local people. Explanation B: It was a college class trip. Conclusion: He had a great time on his educational trip.\n",
    "Answer: Explanation B explains the Conclusion better because the educational trip was more likely a college class trip. Correct explanation is B.\n",
    "\n",
    "The task is to choose a better and more coherent Explanation (B or A) for the Conclusion.\n",
    "Context: {O1} Explanation A: {H1} Explanation B: {H2} Conclusion: {O2}\n",
    "Answer: \"\"\"\n",
    "\n",
    "likeliness_template1 = PromptTemplate(input_variables=[\"O1\", \"O2\", \"H1\", \"H2\"], template=likeliness1)\n",
    "likeliness1_chain = LLMChain(llm=llm, prompt=likeliness_template1, output_key=\"likeliness1\")\n",
    "\n",
    "likeliness2 = \"\"\"The task is to decide which story is more coherent, considering only the information in the story. Story A: {O1} {H1} {O2} Story B: {O1} {H2} {O2}. Which Story (B or A) is more coherent?\"\"\"\n",
    "likeliness_template2 = PromptTemplate(input_variables=[\"O1\", \"O2\", \"H1\", \"H2\"], template=likeliness2)\n",
    "likeliness2_chain = LLMChain(llm=llm, prompt=likeliness_template2, output_key=\"likeliness2\")\n",
    "\n",
    "likeliness3 = \"\"\"The task is to decide which Text is worst explanation for Conclusion ({O2}).\n",
    "Text A is: {O1} {H1} Text B is: {O1} {H2} Which Text (B or A) is the worst explanation for Conclusion and why?\"\"\"\n",
    "likeliness_template3 = PromptTemplate(input_variables=[\"O1\", \"O2\", \"H1\", \"H2\"], template=likeliness3)\n",
    "likeliness3_chain = LLMChain(llm=llm, prompt=likeliness_template3, output_key=\"likeliness3\")\n",
    "\n",
    "\n",
    "ChainOfThought = \"\"\"Select the more probable explanation of given context from a pair of hypothesis choices and give an explanation for the choice.\n",
    "Next, I will give you four examples for test.\n",
    "\n",
    "Context: Jenny wanted cupcakes. She bought them all.\n",
    "Hypothesis choices: A. The bakery only had three cupcakes left. B. The bakery had no cupcakes left.\n",
    "Is the hypothesis A consistent? Yes. Is the hypothesis B consistent? No, Jenny can not buy cupcakes as there are no cupcakes left. \n",
    "The more likely is hypothesis A. The correct hypothesis is A.\n",
    "\n",
    "Context: Jake was at the park. He chased after it but never caught it.\n",
    "Hypothesis choices: A. Jake saw a tree. B. Jake saw a squirrel.\n",
    "Answer: Is the hypothesis A consistent? No. Is the hypothesis B consistent? Yes. \n",
    "The more likely is hypothesis B. The correct hypothesis is B.\n",
    "\n",
    "Context: When I was younger, I went to summer camp. My favorite card game was poker.\n",
    "Hypothesis choices: A. We ate soup every night. B. We played cards a lot.\n",
    "Answer: Is the hypothesis A consistent? Yes. Is the hypothesis B consistent? Yes. \n",
    "The more likely is hypothesis B. The correct hypothesis is B.\n",
    "\n",
    "Context: It was a swelteringly hot day. They all loved the cool treat!\n",
    "Hypothesis choices: A. Everyone was given ice cream. B. Everyone was given soup.\n",
    "Answer: Is the hypothesis A consistent? Yes. Is the hypothesis B consistent? Yes. \n",
    "The more likely is hypothesis A. The correct hypothesis is A.\n",
    "\n",
    "Next, I will give you an example for test.\n",
    "Context: {O1} {O2} \n",
    "Hypothesis choices: A. {H1} B. {H2}\n",
    "Answer: Is the hypothesis A consistent? {answerA}. Is the hypothesis B consistent? {answerB}.\n",
    "The more likely is hypothesis {likeliness}.\n",
    "\"\"\"\n",
    "\n",
    "ChainOfThought_template = PromptTemplate(input_variables=[\"O1\", \"O2\", \"H1\", \"H2\",\"answerA\", \"answerB\", \"likeliness\"], template=ChainOfThought)\n",
    "ChainOfThought_chain = LLMChain(llm=llm, prompt=ChainOfThought_template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc53658-463a-419b-a9e1-881a7a8d4325",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_chain = SequentialChain(chains=[likeliness1_chain, likeliness2_chain, likeliness3_chain], #likeliness_chain, ChainOfThought_chain],\n",
    "                                input_variables=[\"O1\",\"O2\", \"H1\", \"H2\"], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739a1429-0cd6-41de-922b-69bcf01d73f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_prediction(model, chain, samples, save):\n",
    "    answers = []\n",
    "    story_id = []\n",
    "    cA1, cA2, cA3, cA4 = [], [], [], []\n",
    "    cB1, cB2, cB3, cB4 = [],[],[],[]\n",
    "    cC1, cC2, cC3, cC4 = [],[],[],[]\n",
    "    likely1, likely2, likely3 = [], [], []\n",
    "    c = 1\n",
    "\n",
    "    for n in samples.index:\n",
    "        obs1 = samples.obs1[n]\n",
    "        obs2 = samples.obs2[n]\n",
    "        hyp1 = samples.hyp1[n]\n",
    "        hyp2 = samples.hyp2[n]\n",
    "\n",
    "        #print(obs1, hyp1)\n",
    "        #print(obs2)\n",
    "        likely1.append(likeliness1_chain.run({\"O1\":obs1,\"O2\":obs2,\"H1\":hyp1, \"H2\":hyp2}))\n",
    "        likely2.append(likeliness2_chain.run({\"O1\":obs1,\"O2\":obs2,\"H1\":hyp1, \"H2\":hyp2}))\n",
    "        likely3.append(likeliness3_chain.run({\"O1\":obs1,\"O2\":obs2,\"H1\":hyp1, \"H2\":hyp2}))\n",
    "        \"\"\"\n",
    "        consA1 = consistencyA1_chain.run({\"O1\":obs1,\"O2\":obs2,\"H1\":hyp1})\n",
    "        cA1.append(consA1)\n",
    "        cA2.append(consistencyA2_chain.run({\"O1\":obs1,\"O2\":obs2,\"H2\":hyp2}))\n",
    "        cA3.append(consistencyA3_chain.run({\"O1\":obs1,\"O2\":obs2,\"H1\":hyp1}))\n",
    "        cA4.append(consistencyA4_chain.run({\"O1\":obs1,\"O2\":obs2,\"H2\":hyp2}))\n",
    "        #consB = consistencyC_chain.run({\"O1\":obs1,\"O2\":obs2,\"H1\":hyp2})\n",
    "        #cB.append(consB)\n",
    "        cB1.append(consistencyB1_chain.run({\"O1\":obs1,\"O2\":obs2,\"H1\":hyp1}))\n",
    "        cB2.append(consistencyB2_chain.run({\"O1\":obs1,\"O2\":obs2,\"H2\":hyp2}))\n",
    "        cB3.append(consistencyB3_chain.run({\"O1\":obs1,\"O2\":obs2,\"H1\":hyp1}))\n",
    "        cB4.append(consistencyB4_chain.run({\"O1\":obs1,\"O2\":obs2,\"H2\":hyp2}))\n",
    "        cC1.append(consistencyC1_chain.run({\"O1\":obs1,\"O2\":obs2,\"H1\":hyp1}))\n",
    "        cC2.append(consistencyC2_chain.run({\"O1\":obs1,\"O2\":obs2,\"H2\":hyp2}))\n",
    "        cC3.append(consistencyC3_chain.run({\"O1\":obs1,\"O2\":obs2,\"H1\":hyp1}))\n",
    "        cC4.append(consistencyC4_chain.run({\"O1\":obs1,\"O2\":obs2,\"H2\":hyp2}))\n",
    "        \"\"\"\n",
    "\n",
    "        #lik = likeliness_chain.run({\"O1\":obs1,\"O2\":obs2,\"H1\":hyp1, \"H2\":hyp2})\n",
    "        #print(lik)\n",
    "        #likely.append(lik)\n",
    "        #out = chain.run({\"O1\":obs1,\"O2\":obs2,\"H1\":hyp1, \"H2\":hyp2})#, \"likeliness\":lik})\n",
    "        #print('ANSWER', out)\n",
    "        #print('..............')\n",
    "\n",
    "        # with chain i dont know how to get the partial answers to csv\n",
    "        #a = chain.run({\"O1\":obs1,\"O2\":obs2,\"H1\":hyp1, \"H2\":hyp2})\n",
    "\n",
    "        #answers.append(out)\n",
    "        story_id.append(samples.story_id[n])\n",
    "\n",
    "        if c%10 == 0:\n",
    "            pd.DataFrame({'story_id':story_id, \"likely1\":likely1, \"likely2\":likely2, \"likely3\":likely3}).to_csv(save, index=False)\n",
    "            print(c)\n",
    "        c += 1\n",
    "        \n",
    "    pd.DataFrame({'story_id':story_id, \"likely1\":likely1, \"likely2\":likely2, \"likely3\":likely3}).to_csv(save, index=False)\n",
    "    #pd.DataFrame({'story_id':story_id, \"consA1\":cA1, \"consB1\":cB1, \"consC1\":cC1, # 'answer':answers\n",
    "    #                      \"consA2\":cA2, \"consB2\":cB2, \"consC2\":cC2,\n",
    "    #                      \"consA3\":cA3, \"consB3\":cB3, \"consC3\":cC3,\n",
    "    #                      \"consA4\":cA4, \"consB4\":cB4, \"consC4\":cC4}).to_csv(save, index=False)\n",
    "    print('samples answered: ', len(answers))\n",
    "\n",
    "    return answers\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
